
NEED TO MAKE SURE I'VE USED SNAKE_CASE AND NOT CAMEL!!!



==========================

DONE NOT CHECKED

def sanitize_word(word):
    """
    Removes all non ascii characters from a given word
    """    
    newword = ""
    <YOUR-CODE-HERE>
    return(newword)


regex? x

definition of a non-ascii character - e.g. any char with a code >128?

just loop linearly through the word, adding all ascii chars to newword

============================

DONE NOT CHECKED
def parse_line(line):
    """    
    Parses a given line, 
    removes whitespaces, splits into list of sanitize words
    Uses sanitize_word()
    
    HINT: Consider using the "strip()" and "split()" function here
    
    """    
    
    <YOUR-CODE-HERE>
    return(list_of_words)


split() string into a list

strip() each entry to remove any leading and trailing spaces





======================================





def index_file  (filename
                ,filepath
                ,forward_index
                ,invert_index
                ,term_freq
                ,doc_rank
                ):
    """    
    Given a file, indexes it by calculating its:
        forward_index
        term_freq
        doc_rank
    and updates the global invert_index
    """
    start = timer()
    with open(filepath, 'r', encoding="utf-8") as f:
    
    <YOUR-CODE-HERE>           
    
    end = timer()
    print("Time taken to index file: ", filename, " = ", end-start)




Given a file, indexes it by calculating its:
    forward_index
    term_freq
    doc_rank



and updates the global invert_index


While you are reading in the files to create the indices though, you will have to be careful about cleaning up the text to remove special characters and white spaces, and also deal with case sensitivity (recall, the search engine is meant to be case insensitive).

You should use appropriate data structures to store each of these four crucial variables.

forward_index:

    i.Forward Index. This index associates all documents with the associated words.
    E.g.
    ii.anna_karenina.txt: 'the', 'project', 'gutenberg', 'ebook', ...
    second.txt: 'why', 'said', 'the', 'old', ...
    ...
    war_and_peace.txt: 'the', 'project', 'gutenberg', 'ebook', ...




For the data structure; python sets seem to be the best way to do the querying, but a requirement is to be able to dump the data in a textfile. I _think_ working with sets and employing  a routine to parse the file into a set, and to save the text file still meets the spec?

Has the advantage that a set will only allow 1 copy of each word

SOoo... for forward_index - use the following file format for the txt file

name_of_file.txt: item1 item2

Split by filename using an eol delimiter, ';'(?), then split to take filename using the colon character, then import into a set using split on whitespace

2 options - read the text files back and populate a binary search tree
or - investigate, can we store the data in the text files in a useful sorted format?

!!!!!!!Start by generating unsorted text files, and go from there!!!!!!


The forward_index is just a version of term_freq without the frequencies - so that can be created from the term-term_freq
Similarly for invert_index and doc_rank?


OK - so start with term_freq. start reading through a file and add each word to a BST - when get to each word in the source doc we check if it already exists, and add it or update date it's frequency count.

NOOOOO - the moodle examples in Unit 3 (8.1.4) talk about (actually they suggest!!!) using the Python Set function - so we can just add each item to a set, - can we use sets to enumerate the members?

When done we can just make a copy of term_freq, stripping out the frequency data











